# BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects
#### Paper: https://arxiv.org/pdf/2303.14158 
#### Code: https://arxiv.org/pdf/2303.14158](https://github.com/NVlabs/BundleSDF

## Abstract 
- 用于从单目RGBD视频序列中对未知物进行6自由度跟踪，同时进行神经三维重建。 
  - 特点：神经物体场和位姿图优化过程同步学习。 
  - 作用：将信息累计成一个一致的三维表达，同时捕捉了物体的几何形状和外观特征。 
  - 实现方法：维护一个包含位姿信息的动态记忆帧池，用于促进处理流程之间的信息交互。

## Introduction 
- 出发点&问题：CV邻域中两个基本且紧密相关的问题，其一为从单目RGBD视频中对未知物体进行6自由度位姿跟踪，其二为对该物体进行三维重建。然而之前的方法通常假定相机位姿已知或真实的掩膜已知，此外，动态移动的相机捕捉静止物体时，无法实现完整的三维重建，实时的6自由度物体位姿估计与跟踪方法通常需要获取物体的带有纹理的三维模型进行预训练或在线模板匹配。
- 解决方案：假定物体是刚性的，并且需要视频第一帧中的二维物体掩膜，除了这两个要求之外，物体可以自由移动，乃至严重遮挡。一个在线位姿图优化过程、一个用于重建三维形状和外观的并行神经物体场，以及一个促进这两个过程之间信息交互的记忆池。
- 引入了一种混合符号距离函数（SDF）表示法，以应对在以动态物体为中心的场景中，因特别问题所导致的不确定自由空间问题，例如存在噪声的分割结果以及因交互产生的外部遮挡。

## Conclusion
- 是一种从单目RGBD视频中进行6自由度物体跟踪和三维重建的新方法。仅需在初始帧对物体进行分割，通过并行运行的两个线程分别执行在线图优化姿态估计和神经物体场表示，应对诸如快速运动、部分和完全遮挡、缺乏纹理以及镜面高光等挑战性场景。

## Related Work
- 自由度物体姿态估计与跟踪
  - 利用第一帧姿态估计，另外利用时间信息来估计视频中每一帧的物体姿态。
- 同时定位与建图（SLAM）
  - 静态：跟踪相机相对于一个大型静态环境的姿态
  - 动态：通过结合颜色信息的帧模型迭代最近点（ICP）算法、概率数据关联或三维水平集最大化来跟踪动态物体。通过将新跟踪到的姿态与g观察到的RGBD数据聚合，模型会即时进行重建。
  - 本论文方法：利用神经物体场表示，专注于物体中心的设置，能够自动即时融合，同时修正历史跟踪姿态以保持多视图统一性。
- 物体重建
  - 不假设已知相机姿态或真实分割，无需专注在具有丰富纹理或几何线索的静态场景，此方法按照视频流的顺序依次进行处理，不假设对交互主体的特定知识，保证了泛化性。

## Approach

### 1. 粗略位姿初始化 (Coarse Pose Initialization)

#### 目标
为后续的在线姿态图优化提供一个良好的初始位姿估计。

#### 输入
- 当前帧 Ft
- 前一帧 Ft-1（或者记忆池中的参考帧）

#### 流程

1. **物体分割**: 
   使用一个与物体无关的视频分割网络（论文中使用的是 XMem）来获取当前帧 Ft 中物体的 2D 掩膜。这个分割网络不需要物体的先验知识。

2. **特征匹配**:
   使用一个基于 Transformer 的特征匹配网络（论文中使用的是 LoFTR，也可以用 SuperGlue 等其他网络）在当前帧 Ft 和前一帧 Ft-1 的 RGB 图像之间建立特征对应关系。这个特征匹配网络在大规模数据集上进行了预训练，具有较强的泛化能力。

3. **位姿估计**:
   将特征对应关系和对应的深度信息结合起来，利用 RANSAC（随机采样一致性）算法滤除外点，然后使用最小二乘法求解一个 6-DoF 位姿变换 ξt，这个位姿变换表示当前帧 Ft 相对于前一帧 Ft-1 的相对运动。

4. **处理第一帧和丢失跟踪**:
   - 如果是处理视频序列的第一帧, 因为没有前一帧，则当前帧和物体的初始位姿和坐标系直接初始化。
   - 如果因为目标丢失或者严重遮挡而不能计算和上一帧的关系时, 当前帧不会和前一帧进行匹配, 而是和 memory pool 里的所有关键帧匹配, 选取最接近的一帧计算当前位姿。

#### 输出
当前帧 Ft 相对于前一帧 Ft-1（或记忆池中的参考帧）的粗略位姿估计 ξt。

#### key
- 使用与物体无关的分割网络，避免了对特定物体的依赖。
- 使用预训练的特征匹配网络，提高了匹配的准确性和鲁棒性。
- 使用 RANSAC 和最小二乘法，可以有效地处理外点和噪声。
- 当目标丢失后利用和 memory pool 的关键帧对比，保证重新出现的目标可以立即恢复粗略位姿。

![img](res_paper_reading_01/fig01.jpeg)

### 2. 记忆池管理 (Memory Pool Management)

#### 目标
存储一系列关键帧及其位姿信息，用于后续的姿态图优化和神经场学习。记忆池可以有效地保留历史信息，避免灾难性遗忘，并提高跟踪的长期鲁棒性。

#### 数据结构
记忆池 P 是一个关键帧的集合，每个关键帧包含以下信息：
- RGBD 图像
- 物体掩膜
- 位姿（相对于物体坐标系）
- 一个标志位 b(F)，指示该帧的位姿是否已经通过神经物体场进行了优化

#### 流程

1. **初始化**:
   视频序列的第一帧 F0 自动添加到记忆池中，并将物体的坐标系设置为 F0 的相机坐标系。

2. **添加新帧**:
   - 对于每个新帧 Ft，首先计算其粗略位姿 ξt。
   - 然后，使用在线姿态图优化对 ξt 进行优化，得到更新后的位姿 ξ't。
   - 计算 ξ't 与记忆池中所有帧的位姿之间的差异（考虑旋转测地距离，并忽略绕相机光轴的旋转）。
   - 如果 ξ't 与记忆池中所有帧的视角差异都足够大（超过一个阈值），则将 Ft 及其相关信息添加到记忆池中。这个策略保证了记忆池中的帧具有足够的多样性，能够覆盖物体的不同视角。
   
   关于视角距离的定义，忽略图像平面内旋转 (in-plane rotation) 对于保证足够视角信息很有帮助。因为如果加入了过多的只有平面旋转的图片，对于目标 3D 信息的补充很少。

3. **更新位姿**:
   在线姿态图优化和神经物体场学习都会更新记忆池中帧的位姿。如果一个帧的位姿被神经物体场更新过，则将其标志位 b(F) 设置为 TRUE，表示其位姿已经足够精确，后续的在线姿态图优化不再更新它。

4. **大小限制**:
   记忆池的大小是有限的（论文中限制为 K=10），以保证计算效率。当超过了这个数量后, 会根据一定的标准筛选其中重要的关键帧。

#### key
- 记忆池存储了关键帧及其位姿，提供了历史信息。
- 添加新帧的策略基于视角差异，保证了记忆池的多样性。
- 使用标志位来区分位姿是否被神经物体场优化过。
- 记忆池的大小是有限的，以保证计算效率。

### 3. 在线姿态图优化 (Online Pose Graph Optimization)。

**1. 目标与背景**

*   **目标：** 在给定当前帧粗略位姿估计和 Memory Pool 中关键帧信息的情况下，精细地优化当前帧的位姿，同时也可以选择性地优化 Memory Pool 中部分关键帧的位姿。
*   **背景：**
    *   直接使用粗略位姿估计（来自特征匹配和 RANSAC）精度有限，可能存在误差。
    *   随着时间的推移，误差会累积，导致漂移 (drift)。
    *   在线姿态图优化可以利用多帧之间的几何约束来减少误差，提高位姿估计的精度和鲁棒性。
    *   类似于 SLAM (Simultaneous Localization and Mapping) 中的后端优化。

**2. 关键元素**

*   **Pose Graph (位姿图):**
    *   **节点 (Nodes):**
        *   当前帧 *Ft*
        *   从 Memory Pool 中选择的 *K* 个关键帧 
        *    Memory Pool 中的哪些帧被选中，在memory pool 的选择有描述。

    *   **边 (Edges):**
        *   表示相邻节点之间的相对位姿约束。
        *   每条边都关联一个损失函数，衡量相对位姿估计与观测数据之间的差异。

*   **Memory Pool 的选择:**
    *   目标：为了提高计算效率, 只计算其中 K 个关键帧和当前帧的关系,而不是整个memory pool。选择与当前帧视角最相关的 *K* 个关键帧。
    *   过程：
        1.  计算 Memory Pool 中每个关键帧的点云法向量。
        2.  计算法向量与当前帧观察方向的点积。
        3.  如果点积大于一个阈值（表明可见性），则计算该关键帧与当前帧的视角差异（旋转测地距离，忽略绕相机光轴的旋转）。
        4.  选择视角差异最小的 *K* 个关键帧。

*   **优化变量:**
    *    当前帧 *Ft* 的位姿 ξt
    *   选定的 *K* 个关键帧的位姿（Memory Pool 中关键帧位姿的初始值可能来自历史的在线位姿图优化，也可能来自神经物体场的优化，取决于 `b(F)` 标志位）。

**3. 损失函数 (Loss Function)**

在线姿态图优化的目标是最小化一个总的损失函数，该损失函数由多个损失项加权组合而成：

```
Lpg = WsLs(t) + Σ [WfLf(i, j) + WpLp(i, j)]
```
对每一项的解释：

*   **Ls(t) (Unary Loss - SDF Loss):**
    *   **作用:** 利用神经物体场提供的 SDF 信息来约束当前帧的位姿。
    *   **计算:** 在当前帧的图像中采样一系列像素，根据当前帧的位姿将这些像素反投影到 3D 空间，然后查询神经物体场 (Neural Object Field) 得到这些 3D 点的 SDF 值。Ls(t) 衡量这些 SDF 值的绝对值之和（或加权和）。

        ```
        Ls(t) = Σ ρ(Ω(ξt⁻¹(π⁻¹(p))))
        ```

        其中：
        *   `p` 是当前帧图像中的像素坐标。
        *   `π⁻¹(p)` 是将像素 `p` 反投影到 3D 空间（使用深度信息）。
        *   `ξt⁻¹` 是将 3D 点从世界坐标系变换到当前帧相机坐标系的变换矩阵（当前帧位姿的逆变换）。
        *   `Ω(·)` 是神经物体场（SDF 函数）。
        *   `ρ(·)` 是 Huber 损失函数。
    *    只有当 Neural Object Field 初始化训练完成后,才会启用。
        *    作用:  当完成首次训练,神经场可以提供一个比较好的初值,此时 SDF loss可以大大帮助优化.

*   **Lf(i, j) (Pairwise Loss - Feature Matching Loss):**
    *   **作用:** 利用帧之间的特征匹配信息来约束相对位姿。
    *   **计算:** 在当前帧 *Fi* 和关键帧 *Fj* 之间找到匹配的特征点对，Lf(i, j) 衡量这些匹配点对在 3D 空间中的距离。

        ```
        Lf(i, j) = (1 / |Ci,j|) * Σ ρ(||pm - ξi⁻¹ ξj pn||²)
        ```

        其中：
        *   `Ci,j` 是帧 *Fi* 和 *Fj* 之间匹配的特征点对集合。
        *   `pm` 和 `pn` 是匹配的特征点对在 3D 空间中的坐标（通过反投影得到）。
        *   `ξi⁻¹ ξj` 是将点从帧 *Fj* 坐标系变换到帧 *Fi* 坐标系的变换矩阵。
        *   `ρ(·)` 是 Huber 损失函数。
        * 特征匹配使用的和粗略位姿计算使用的同一个网络. 不过会重新计算.

*   **Lp(i, j) (Pairwise Loss - Point-to-Plane Loss):**
    *   **作用:** 利用帧之间的重叠区域来约束相对位姿，这是经典的 ICP (Iterative Closest Point) 算法中常用的损失项。
    *   **计算:** 在当前帧 *Fi* 和关键帧 *Fj* 之间找到重叠区域（通过像素反投影和深度测试），Lp(i, j) 衡量这些重叠区域中点到面的距离。

        ```
        Lp(i, j) = Σ ρ(ni(p) ⋅ (ξj⁻¹ ξi π⁻¹(p) - p))
        ```

        其中：
        *   `p` 是帧 *Fi* 图像中重叠区域的像素坐标。
        *   `π⁻¹(p)` 是将像素 `p` 反投影到 3D 空间。
        *    ξi 和 ξj 的逆和Lf(i, j) 定义的相反, 这是因为这里使用的是image i 和 j, 也就是对应于pose i和j。
        *   `ξj⁻¹ ξi` 是将点从帧 *Fi* 坐标系变换到帧 *Fj* 坐标系的变换矩阵。
        *   `ni(p)` 是帧 *Fi* 中像素 `p` 对应的 3D 点的法向量。
        *   `ρ(·)` 是 Huber 损失函数。

*   **权重 (Weights):** 论文中，Ws、Wf 和 Wp 都设置为 1。

**4. 优化过程**

*   **优化方法:** 使用 Gauss-Newton 算法迭代地优化位姿，最小化总损失函数 Lpg。
*   **迭代更新:** 在每次迭代中，根据损失函数的梯度更新当前帧和选定关键帧的位姿。
*   **位姿表示:** 位姿使用 Lie Algebra (李代数) 表示，这是一种常用的位姿表示方式，可以方便地进行优化。
*  在CUDA实现,加快计算。
**5. 输出**
    * 当前帧的优化位姿。
   *   选择的Memory Pool 关键帧的优化位姿（取决于 `b(F)` 标志位，如果标志位为 TRUE，则不更新）。

**总结:**
 * pose graph的构建：
  * 节点： 当前帧 +  Memory Pool 选择的关键帧
  *  边： 帧之间的位姿
* 损失函数：
    *   Ls(t): 来自神经场的 SDF 损失 (单帧)
    *   Lf(i,j): 特征匹配损失 (双帧)
    *   Lp(i,j): 点到平面损失 (双帧)
*  使用Gauss-Newton进行优化
*  在线位姿图优化和神经物体场的训练是相互促进的：
        *   在线位姿图优化为神经物体场提供更准确的位姿信息。
        *   神经物体场为在线位姿图优化提供 SDF 损失项，进一步提高位姿估计的精度。


### 4. 神经物体场 (Neural Object Field)

**1. 神经物体场表示 (Object Field Representation)**

*   **核心思想:** 使用两个神经网络来隐式地表示物体的几何形状和外观。
    *   **几何函数 (Geometry Function) Ω: x → s:**
        *   输入：3D 点坐标 **x** ∈ R³
        *   输出：带符号距离值 (Signed Distance Value) s ∈ R，表示该点到物体表面的距离（内部为负，外部为正，表面为 0）
        *   网络结构：一个简单的多层感知机 (MLP)，论文中使用的是两层 MLP，隐藏层维度为 64，使用 ReLU 激活函数（除了最后一层）。
    *   **外观函数 (Appearance Function) Φ: (f(x), n, d) → c:**
        *   输入：
            *   几何网络的中间特征向量 **f(x)** ∈ R³
            *   点法向量 **n** ∈ R³
            *   视线方向 **d** ∈ R³
        *   输出：颜色 **c** ∈ R³，表示该点在给定视角下的颜色
        *   网络结构：一个简单的 MLP，论文中使用的是三层 MLP，隐藏层维度为 64，使用 ReLU 激活函数（除了最后一层使用 Sigmoid 函数将颜色值映射到 [0, 1]）。

*   **输入编码:**
    *   对于 3D 点坐标 **x**，使用多分辨率哈希编码 (Multi-resolution Hash Encoding) 进行输入编码。这是一种可学习的编码方式，能够有效地表示高频细节。论文中使用了 4 个级别的哈希编码，特征向量的数量从 16 到 128 不等，每个级别的特征维度为 2，哈希表大小为 2²²。
    *   对于法向量 **n** 和视线方向 **d**，使用低阶球谐函数 (Spherical Harmonics) 进行编码（论文中使用的是 2 阶球谐函数）。这有助于防止过拟合，特别是对于旋转的过拟合。

*   **法向量计算:**
    *   一个点 **x** 的法向量 **n(x)** 可以通过对 SDF 求梯度得到：**n(x)** = ∇Ω(**x**) = ∂Ω(**x**) / ∂**x**。
    *   论文中使用 PyTorch 的自动微分功能来计算梯度。

*   **物体表面:**
    *   物体的表面 S 可以通过 SDF 的零水平集 (Zero Level Set) 来表示：S = {**x** ∈ R³ | Ω(**x**) = 0}。

**2. 渲染 (Rendering)**

*   **目标:** 给定一个相机位姿 ξ 和一个像素，渲染出该像素对应的颜色。
*   **流程:**
    1.  **光线生成:** 从相机光心 **o(r)** 发出一条射线 **r**，其方向为 **d(r)**，这两个量都取决于相机位姿 ξ。
    2.  **采样:** 沿射线 **r** 采样一系列 3D 点：**xi(r)** = **o(r)** + ti**d(r)**，其中 ti ∈ R⁺ 控制采样点的位置。
    3.  **积分:** 使用体渲染 (Volume Rendering) 技术来计算射线的颜色 **c(r)**：

        ```
        c(r) = ∫[z(r)-λ, z(r)+0.5λ] w(xi) Φ(f(xi), n(xi), d(xi)) dt
        ```

        其中：
        *   z(r) 是深度图像中该射线的深度值。
        *   λ 是截断距离。
        *   w(xi) 是一个钟形概率密度函数，定义如下：

            ```
            w(xi) = 1 / (1 + exp(-αΩ(xi))) * 1 / (1 + exp(αΩ(xi)))
            ```

            其中 α 是一个常数，用于调节概率密度函数的软度。w(xi) 在物体表面附近达到最大值。
        *    积分范围：
              *  没有从负无穷到正无穷积分,是因为考虑到在动态目标追踪问题中，很多时候只能获得目标的部分视角，而过多的无效区域会导致神经场倾向于过拟合一个平均结果。
               * 尤其是在 BundleSDF 中,在每次训练前,会将所有的Memory Pool的点和位姿信息整合到一起用于Octree构建. 而这个Octree的空间分布很大程度取决于目标物体的运动,为了降低积分的范围.

*   **高效采样 (Efficient Hierarchical Ray Sampling):**
    *   **Octree:** 为了提高采样效率，首先使用一个 Octree 数据结构来表示场景。Octree 是通过将 Memory Pool 中所有帧的点云合并而成的。
    *   **分层采样:**
        1.  首先，在 Octree 的占用体素 (Occupied Voxel) 内均匀采样 N 个点，采样范围限定在 z(r) + 0.5λ 以内。
        2.  然后，在深度值 z(r) 附近使用正态分布 N(z(r), λ²) 采样 N' 个点。
        3. 论文使用一个自定义的 CUDA 核函数来跳过 Octree 中空的体素，避免了不必要的采样. 

        总采样点数为 N + N'。这种采样策略可以在物体表面附近分配更多的采样点，提高渲染质量。
* **和一般神经场(NeRF)不同的采样**
  * 一般神经场利用前一次的预测 SDF 信息重要性采样(importance sampling)。
  * BundleSDF 采样和depth更相关。
  * 使用固定数量的采样.

**3. 混合 SDF 建模 (Hybrid SDF Modeling)**

*   **背景:** 在动态场景中，由于分割噪声、外部遮挡和深度缺失等原因，获取的深度信息往往是不准确的。为了处理这些不确定性，论文提出了一种混合 SDF 模型，将空间划分为三个区域：
    *   **不确定自由空间 (Uncertain Free Space):**
        *   对应于分割掩膜中的背景区域，或者深度值缺失的区域。
        *   这些区域的观测是不可靠的。
        *   对于这些区域中的点，我们不希望完全忽略它们，也不希望完全信任它们。因此，我们为它们分配一个小的正值 ε，表示它们可能位于物体外部。

    *   **空闲空间 (Empty Space):**
        *   位于深度值之前的区域（到截断距离为止）。
        *   这些区域几乎可以确定是位于物体外部的。
        *   鼓励稀疏性,因为大概率是空的.
    *   **近表面空间 (Near-surface Space):**
        *   位于深度值之后的一小段区域（到 z(r) + 0.5λ 为止），用于建模自遮挡。
        *   这个区域对于学习 SDF 的符号翻转和零水平集至关重要。
        *   使用投影近似的方式来计算近表面 SDF.
**简单总结**:把场景分成前景,背景和uncertain 区域, 然后对于每个点:

*   背景: 这些点深度为空或者在mask 外面. 因为这部分不可靠,我们使用一个自定义小的值来表达它,让它拥有被纠正的可能
*   空区域: 深度图提供了很好的指示: 这个区域在目标前方, 因此为空. 采用 L1 损失计算.
*   近表面: 位于depth map z(r) 后面很小一部分区域。 使用L_surf 公式进行。

**4. 训练 (Training)**

*   **训练数据:** Memory Pool 中的关键帧及其位姿。
*   **损失函数:**
    ```
    L = WuLu + WeLe + WsurfLsurf + WcLc + WeikLeik
    ```
    其中：
    *   Lu, Le, Lsurf 分别是不确定自由空间、空闲空间和近表面空间的 SDF 损失。
    *   Lc 是颜色损失，用于监督外观函数的学习。
    *   Leik 是 Eikonal 正则化项，用于约束 SDF 的梯度范数为 1。

*   **优化目标:** 优化多分辨率哈希编码、几何函数 Ω、外观函数 Φ 以及 Memory Pool 中关键帧的位姿。
*   **训练过程:**
    *   神经物体场在一个单独的线程中进行训练，与在线位姿跟踪并行。
    *   每个训练周期开始时，从 Memory Pool 中获取所有关键帧及其位姿。
    *   训练收敛后，更新 Memory Pool 中关键帧的位姿，并将训练好的 SDF 模型用于后续的在线姿态图优化。



